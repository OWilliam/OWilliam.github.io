---
title: Waiting for baby Eddie
---

# 8th May 2017

Today has mostly been spent waiting for little Eddie to appear and being sat upon by Bilbo the cat.
I've started reading [the deep learning book](http://www.deeplearningbook.org/) and am running little
character-based RNN models based on [Karpathy's execellent explanation](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) and
the code [here](https://gist.github.com/karpathy/d4dee566867f8291f086).

I've converted the code to run under Python 3 and also written a C++ direct translation to see if it would be any faster than the Python version.
It isn't. Well, the speed depends completely on the speed of matrix dot-product operations. When I use [OpenBLAS](http://www.openblas.net/) on my Windows
box, the optimised (Visual Studio /02 switch) C++ version of the RNN is about as fast as the Python version. Maybe there are faster BLAS's out there? Whatever,
using C++ give us no appreciable performance advantage, so the flexibility of using Python makes it the best choice for this work and I'll not take
the C++ code any further. Obviously, using CUDA on GPUs would make things much faster, so I'll see if I can get TensorFlow running on an AWS GPU instance
when I get time. In the mean time, I'll ponder more algorithmic approaches for speeding up the learning of text models.
